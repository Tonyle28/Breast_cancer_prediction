import streamlit as st
import pandas as pd
import joblib
import numpy as np
import pickle
import google.generativeai as genai

def load_model():
    try:
        model = joblib.load('breast_cancer_model.pkl')
        scaler = joblib.load('scaler.pkl')
        feature_names = joblib.load('feature_names.pkl')
        outlier_bounds = joblib.load('outlier_bounds.pkl')
        return model, scaler, feature_names,outlier_bounds
    except FileNotFoundError:
        st.error("KhÃ´ng tÃ¬m tháº¥y file model. Vui lÃ²ng Ä‘áº£m báº£o cÃ¡c file .pkl Ä‘Ã£ Ä‘Æ°á»£c táº¡o.")
        return None, None, None,None


def get_clean_data():
    try:
        data = pd.read_csv("breast-cancer.csv")
        data = data.drop('id', axis=1)
        data['diagnosis'] = np.where(data['diagnosis'] == 'M', 1, 0)
        return data
    except:
        return None


def add_sidebar():
    st.sidebar.header('ThÃ´ng sá»‘ cá»§a táº¿ bÃ o')
    
    # Láº¥y dá»¯ liá»‡u Ä‘á»ƒ xÃ¡c Ä‘á»‹nh min/max 
    data = get_clean_data()
    
    slider_labels = [
        ("BÃ¡n kÃ­nh (trung bÃ¬nh)", "radius_mean"),
        ("Äá»™ nhÃ¡m (trung bÃ¬nh)", "texture_mean"),
        ("Chu vi (trung bÃ¬nh)", "perimeter_mean"),
        ("Diá»‡n tÃ­ch (trung bÃ¬nh)", "area_mean"),
        ("Äá»™ mÆ°á»£t (trung bÃ¬nh)", "smoothness_mean"),
        ("Äá»™ nÃ©n (trung bÃ¬nh)", "compactness_mean"),
        ("Äá»™ lÃµm (trung bÃ¬nh)", "concavity_mean"),
        ("Äiá»ƒm lÃµm (trung bÃ¬nh)", "concave points_mean"), #
        ("Äá»™ Ä‘á»‘i xá»©ng (trung bÃ¬nh)", "symmetry_mean"),
        ("Chiá»u fractal (trung bÃ¬nh)", "fractal_dimension_mean"),
        ("BÃ¡n kÃ­nh (sai sá»‘)", "radius_se"),
        ("Äá»™ nhÃ¡m (sai sá»‘)", "texture_se"),
        ("Chu vi (sai sá»‘)", "perimeter_se"),
        ("Diá»‡n tÃ­ch (sai sá»‘)", "area_se"), #
        ("Äá»™ mÆ°á»£t (sai sá»‘)", "smoothness_se"),
        ("Äá»™ nÃ©n (sai sá»‘)", "compactness_se"),
        ("Äá»™ lÃµm (sai sá»‘)", "concavity_se"),
        ("Äiá»ƒm lÃµm (sai sá»‘)", "concave points_se"),
        ("Äá»™ Ä‘á»‘i xá»©ng (sai sá»‘)", "symmetry_se"),
        ("Chiá»u fractal (sai sá»‘)", "fractal_dimension_se"),
        ("BÃ¡n kÃ­nh (tá»‡ nháº¥t)", "radius_worst"), #
        ("Äá»™ nhÃ¡m (tá»‡ nháº¥t)", "texture_worst"), #
        ("Chu vi (tá»‡ nháº¥t)", "perimeter_worst"),
        ("Diá»‡n tÃ­ch (tá»‡ nháº¥t)", "area_worst"), #
        ("Äá»™ mÆ°á»£t (tá»‡ nháº¥t)", "smoothness_worst"), #
        ("Äá»™ nÃ©n (tá»‡ nháº¥t)", "compactness_worst"),
        ("Äá»™ lÃµm (tá»‡ nháº¥t)", "concavity_worst"),
        ("Äiá»ƒm lÃµm (tá»‡ nháº¥t)", "concave points_worst"), #
        ("Äá»™ Ä‘á»‘i xá»©ng (tá»‡ nháº¥t)", "symmetry_worst"), #
        ("Chiá»u fractal (tá»‡ nháº¥t)", "fractal_dimension_worst")
    ]

    input_dict = {}
    for label, key in slider_labels:
       col_data = data[key]
       input_dict[key] = st.sidebar.slider(
            label,
            min_value=float(col_data.min()),
            max_value=float(col_data.max()) ,
            value=float(col_data.mean()),
            format="%.4f")
    return input_dict


def get_scaled_values(input_dict, scaler):
    input_df = pd.DataFrame([input_dict])
    # Scale dá»¯ liá»‡u
    scaled_array = scaler.transform(input_df)
    
    return scaled_array


def add_predictions(input_data, model, scaler,outlier_bounds):
    input_df = pd.DataFrame([input_data])
    try:
        # Xá»­ lÃ½ outliner
        clipped_features = []
        for col in input_df.columns:
            lower = outlier_bounds[col]['lower']
            upper = outlier_bounds[col]['upper']
            
            original_value = input_df[col].values[0]
            clipped_value = np.clip(original_value, lower, upper)
            
            input_df[col] = clipped_value

        # Chuáº©n hÃ³a dá»¯ liá»‡u
        scaled_data = scaler.transform(input_df)
        
        # Dá»± Ä‘oÃ¡n
        prediction = model.predict(scaled_data)
        probability = model.predict_proba(scaled_data)
        
        st.subheader("ğŸ”¬ Káº¿t quáº£ dá»± Ä‘oÃ¡n")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if prediction[0] == 0:
                st.success("âœ… **LÃ€NH TÃNH (Benign)**")
                st.markdown(f"### Äá»™ tin cáº­y: {probability[0][0]*100:.2f}%")
            else:
                st.error("âš ï¸ **ÃC TÃNH (Malignant)**")
                st.markdown(f"### Äá»™ tin cáº­y: {probability[0][1]*100:.2f}%")
        
        with col2:
            # Biá»ƒu Ä‘á»“ xÃ¡c suáº¥t
            prob_df = pd.DataFrame({
                'Loáº¡i': ['LÃ nh tÃ­nh', 'Ãc tÃ­nh'],
                'XÃ¡c suáº¥t (%)': [probability[0][0]*100, probability[0][1]*100]
            })
            st.bar_chart(prob_df.set_index('Loáº¡i'))
        
        # Hiá»ƒn thá»‹ chi tiáº¿t xÃ¡c suáº¥t
        st.write("---")
        st.write("**ğŸ“Š PhÃ¢n bá»‘ xÃ¡c suáº¥t chi tiáº¿t:**")
        
        metric_col1, metric_col2 = st.columns(2)
        with metric_col1:
            st.metric(
                label="ğŸŸ¢ LÃ nh tÃ­nh (Benign)", 
                value=f"{probability[0][0]*100:.2f}%"
            )
        with metric_col2:
            st.metric(
                label="ğŸ”´ Ãc tÃ­nh (Malignant)", 
                value=f"{probability[0][1]*100:.2f}%"
            )
        
        st.write("---")
        st.warning("""
        âš ï¸ **LÆ¯U Ã QUAN TRá»ŒNG:**
        - Káº¿t quáº£ nÃ y chá»‰ mang tÃ­nh cháº¥t tham kháº£o tá»« mÃ´ hÃ¬nh AI
        - Model cÃ³ F1 ~96-98% trÃªn táº­p test
        - Vui lÃ²ng tham kháº£o Ã½ kiáº¿n bÃ¡c sÄ© chuyÃªn khoa Ä‘á»ƒ cÃ³ cháº©n Ä‘oÃ¡n chÃ­nh xÃ¡c
        - KhÃ´ng tá»± Ã½ Ä‘iá»u trá»‹ dá»±a trÃªn káº¿t quáº£ nÃ y
        """)
        
        # ThÃ´ng tin vá» model
        with st.expander("â„¹ï¸ ThÃ´ng tin vá» mÃ´ hÃ¬nh"):
            st.write("""
            **MÃ´ hÃ¬nh:** Logistic Regression
            
            **ThÃ´ng sá»‘:**
            - Solver: liblinear
            - Penalty: L2 (Ridge)
            - Regularization (C): 0.1
            - Class Weight: Balanced
            
            **Hiá»‡u suáº¥t:**
            - Cross-Validation F1: ~96-98%
            - Test F1: ~96-98%
            
            **Dá»¯ liá»‡u huáº¥n luyá»‡n:**
            - Dataset: Breast Cancer Wisconsin
            - Sá»‘ máº«u: 569 bá»‡nh nhÃ¢n
            - Sá»‘ Ä‘áº·c trÆ°ng: 30 Ä‘áº·c trÆ°ng tá»« hÃ¬nh áº£nh nhÃ¢n táº¿ bÃ o
            """)
        
    except Exception as e:
        st.error(f"âŒ CÃ³ lá»—i xáº£y ra khi dá»± Ä‘oÃ¡n: {str(e)}")
        st.info("ğŸ’¡ Vui lÃ²ng kiá»ƒm tra láº¡i dá»¯ liá»‡u Ä‘áº§u vÃ o vÃ  cÃ¡c file model")


def main():
    st.set_page_config(
        page_title="Dá»± Ä‘oÃ¡n Ung ThÆ° VÃº",
        layout="wide", 
        page_icon="ğŸ©º"
    )

    # Load model
    model, scaler, feature_names,outlier_bounds = load_model()

    # Header
    with st.container():
        st.title("ğŸ©º Dá»± Ä‘oÃ¡n Ung thÆ° vÃº sá»­ dá»¥ng mÃ´ hÃ¬nh Logistic Regression")
        st.write("""
        á»¨ng dá»¥ng sá»­ dá»¥ng bá»™ dá»¯ liá»‡u **Breast Cancer Wisconsin** Ä‘á»ƒ há»— trá»£ cháº©n Ä‘oÃ¡n sá»›m ung thÆ° vÃº, 
        cÄƒn bá»‡nh phá»• biáº¿n hÃ ng Ä‘áº§u á»Ÿ ná»¯ giá»›i. Báº±ng cÃ¡ch phÃ¢n tÃ­ch cÃ¡c Ä‘áº·c trÆ°ng nhÃ¢n táº¿ bÃ o qua 
        thuáº­t toÃ¡n **Logistic Regression**, há»‡ thá»‘ng giÃºp chuyá»ƒn hÃ³a cÃ¡c chá»‰ sá»‘ y khoa phá»©c táº¡p 
        thÃ nh káº¿t quáº£ dá»± Ä‘oÃ¡n khá»‘i u lÃ nh tÃ­nh hoáº·c Ã¡c tÃ­nh.
        """)
        st.info("ğŸ’¡ **HÆ°á»›ng dáº«n:** Äiá»u chá»‰nh cÃ¡c thanh trÆ°á»£t bÃªn trÃ¡i Ä‘á»ƒ nháº­p cÃ¡c chá»‰ sá»‘ tá»« káº¿t quáº£ xÃ©t nghiá»‡m, sau Ä‘Ã³ nháº¥n nÃºt 'Dá»± Ä‘oÃ¡n' bÃªn dÆ°á»›i.")
    
    # Táº¡o sidebar
    sidebar_data = add_sidebar()
    
    # Main content
    col1, col2 = st.columns([1, 1])

    with col1:
        st.subheader("ğŸ“Š Dá»¯ liá»‡u Ä‘Ã£ nháº­p")
        st.write("Má»™t sá»‘ giÃ¡ trá»‹ chÃ­nh tá»« thanh trÆ°á»£t:")
        st.write("* LÆ°u Ã½ Ä‘iá»u chá»‰nh cÃ¡c biáº¿n: Nhá»¯ng biáº¿n nhÆ° diá»‡n tÃ­ch, chu vi, bÃ¡n kÃ­nh, káº¿t cáº¥u cÃ³ thá»ƒ gÃ¢y áº£nh hÆ°á»Ÿng lá»›n hÆ¡n vá»›i mÃ´ hÃ¬nh !!!")
        
        # Hiá»ƒn thá»‹ cÃ¡c giÃ¡ trá»‹ quan trá»ng
        st.metric("Äiá»ƒm lÃµm (trung bÃ¬nh)", f"{sidebar_data['concave points_mean']:.4f}")
        st.metric("Diá»‡n tÃ­ch (sai sá»‘)", f"{sidebar_data['area_se']:.4f}")
        st.metric("BÃ¡n kÃ­nh (tá»‡ nháº¥t)", f"{sidebar_data['radius_worst']:.4f}")
        st.metric("Äá»™ nhÃ¡m (tá»‡ nháº¥t)", f"{sidebar_data['texture_worst']:.4f}")
        st.metric("Diá»‡n tÃ­ch (tá»‡ nháº¥t)", f"{sidebar_data['area_worst']:.4f}")
        st.metric("Äá»™ mÆ°á»£t (tá»‡ nháº¥t)", f"{sidebar_data['smoothness_worst']:.4f}")
        st.metric("Äiá»ƒm lÃµm (tá»‡ nháº¥t)", f"{sidebar_data['concave points_worst']:.4f}")
        st.metric("Äá»™ Ä‘á»‘i xá»©ng (tá»‡ nháº¥t)", f"{sidebar_data['symmetry_worst']:.4f}")
        
    with col2:
        st.subheader("ğŸ“‹ ThÃ´ng tin")
        st.write(f" * Tá»•ng sá»‘ biáº¿n Ä‘Ã£ nháº­p: **{len(sidebar_data)}**")
        st.write(f" * Model Ä‘Ã£ load: **Logistic Regression**")
        st.write(f" * Scaler Ä‘Ã£ load: **StandardScaler**")
        st.write(f" * F1 cá»§a model: **~96-98%**")
    
    # NÃºt dá»± Ä‘oÃ¡n
    st.write("---")
    if st.button("ğŸ” Dá»° ÄOÃN Káº¾T QUáº¢", type="primary", use_container_width=True):
        with st.spinner("â³ Äang phÃ¢n tÃ­ch dá»¯ liá»‡u vÃ  dá»± Ä‘oÃ¡n..."):
            add_predictions(sidebar_data, model, scaler, outlier_bounds)


if __name__ == "__main__":
    main()